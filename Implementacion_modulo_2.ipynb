{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d823bd625b80bfc8",
   "metadata": {},
   "source": [
    "# Implementación Módulo 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1cf67b5dfb0e00c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:03.394510Z",
     "start_time": "2024-12-12T19:09:03.388151Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a11a2e6865b5004",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:08.382790Z",
     "start_time": "2024-12-12T19:09:03.422837Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"processed_df.csv\", sep=\";\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca6b3c1383efe2",
   "metadata": {},
   "source": [
    "Vemos que algunos posts al hacer el preprocesamiento como tenían solo caracteres no alfanumericos ahora contienen Nan.\n",
    "\n",
    "Hay que eliminarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d84619bb0b9e390d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:08.550760Z",
     "start_time": "2024-12-12T19:09:08.417076Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_date       0\n",
       "subreddit          0\n",
       "title              0\n",
       "author             0\n",
       "full_link          0\n",
       "score              0\n",
       "post            1962\n",
       "sentiment          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dba4b5400e3e480c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:08.715552Z",
     "start_time": "2024-12-12T19:09:08.647915Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>full_link</th>\n",
       "      <th>score</th>\n",
       "      <th>post</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>446</th>\n",
       "      <td>2011-02-11 08:04:10</td>\n",
       "      <td>statistics</td>\n",
       "      <td>weighting with r</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>https://www.reddit.com/r/statistics/comments/f...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>2011-04-11 17:54:51</td>\n",
       "      <td>datasets</td>\n",
       "      <td>Where can i download netflix database for my e...</td>\n",
       "      <td>Exibus</td>\n",
       "      <td>https://www.reddit.com/r/datasets/comments/gnf...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>2011-12-04 19:38:20</td>\n",
       "      <td>AskStatistics</td>\n",
       "      <td>If correlation is not necessarily causation, h...</td>\n",
       "      <td>JimJamieJames</td>\n",
       "      <td>https://www.reddit.com/r/AskStatistics/comment...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>2011-12-14 18:29:01</td>\n",
       "      <td>statistics</td>\n",
       "      <td>If you have a series of points that are better...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>https://www.reddit.com/r/statistics/comments/n...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2400</th>\n",
       "      <td>2012-04-02 20:39:37</td>\n",
       "      <td>datasets</td>\n",
       "      <td>Archives bends under rush for 1940 census reco...</td>\n",
       "      <td>berlinbrown</td>\n",
       "      <td>https://www.reddit.com/r/datasets/comments/rps...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272822</th>\n",
       "      <td>2022-05-01 01:01:57</td>\n",
       "      <td>artificial</td>\n",
       "      <td>Sentient microwave develops PTSD, attempts to ...</td>\n",
       "      <td>Nekrofeeelyah</td>\n",
       "      <td>https://www.reddit.com/r/artificial/comments/u...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273079</th>\n",
       "      <td>2022-05-02 13:47:19</td>\n",
       "      <td>datascience</td>\n",
       "      <td>Deep Neural Networks to Detect Weeds from Crop...</td>\n",
       "      <td>SpecificAd3444</td>\n",
       "      <td>https://www.reddit.com/r/datascience/comments/...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273477</th>\n",
       "      <td>2022-05-04 15:34:14</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>Dockerizing an Apache Spark Standalone Cluster</td>\n",
       "      <td>ramses-coraspe</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273531</th>\n",
       "      <td>2022-05-04 20:01:57</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>Build an ETL pipeline with Apache Airflow and ...</td>\n",
       "      <td>ramses-coraspe</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274029</th>\n",
       "      <td>2022-05-07 15:54:37</td>\n",
       "      <td>dataengineering</td>\n",
       "      <td>Apache spark</td>\n",
       "      <td>ramses-coraspe</td>\n",
       "      <td>https://www.reddit.com/r/dataengineering/comme...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1962 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_date        subreddit  \\\n",
       "446     2011-02-11 08:04:10       statistics   \n",
       "605     2011-04-11 17:54:51         datasets   \n",
       "1631    2011-12-04 19:38:20    AskStatistics   \n",
       "1698    2011-12-14 18:29:01       statistics   \n",
       "2400    2012-04-02 20:39:37         datasets   \n",
       "...                     ...              ...   \n",
       "272822  2022-05-01 01:01:57       artificial   \n",
       "273079  2022-05-02 13:47:19      datascience   \n",
       "273477  2022-05-04 15:34:14  dataengineering   \n",
       "273531  2022-05-04 20:01:57  dataengineering   \n",
       "274029  2022-05-07 15:54:37  dataengineering   \n",
       "\n",
       "                                                    title          author  \\\n",
       "446                                      weighting with r       [deleted]   \n",
       "605     Where can i download netflix database for my e...          Exibus   \n",
       "1631    If correlation is not necessarily causation, h...   JimJamieJames   \n",
       "1698    If you have a series of points that are better...       [deleted]   \n",
       "2400    Archives bends under rush for 1940 census reco...     berlinbrown   \n",
       "...                                                   ...             ...   \n",
       "272822  Sentient microwave develops PTSD, attempts to ...   Nekrofeeelyah   \n",
       "273079  Deep Neural Networks to Detect Weeds from Crop...  SpecificAd3444   \n",
       "273477     Dockerizing an Apache Spark Standalone Cluster  ramses-coraspe   \n",
       "273531  Build an ETL pipeline with Apache Airflow and ...  ramses-coraspe   \n",
       "274029                                       Apache spark  ramses-coraspe   \n",
       "\n",
       "                                                full_link  score post  \\\n",
       "446     https://www.reddit.com/r/statistics/comments/f...      1  NaN   \n",
       "605     https://www.reddit.com/r/datasets/comments/gnf...      1  NaN   \n",
       "1631    https://www.reddit.com/r/AskStatistics/comment...      0  NaN   \n",
       "1698    https://www.reddit.com/r/statistics/comments/n...      1  NaN   \n",
       "2400    https://www.reddit.com/r/datasets/comments/rps...      0  NaN   \n",
       "...                                                   ...    ...  ...   \n",
       "272822  https://www.reddit.com/r/artificial/comments/u...      1  NaN   \n",
       "273079  https://www.reddit.com/r/datascience/comments/...      1  NaN   \n",
       "273477  https://www.reddit.com/r/dataengineering/comme...      1  NaN   \n",
       "273531  https://www.reddit.com/r/dataengineering/comme...      1  NaN   \n",
       "274029  https://www.reddit.com/r/dataengineering/comme...      1  NaN   \n",
       "\n",
       "        sentiment  \n",
       "446             1  \n",
       "605             1  \n",
       "1631            1  \n",
       "1698            1  \n",
       "2400            0  \n",
       "...           ...  \n",
       "272822          0  \n",
       "273079          0  \n",
       "273477          0  \n",
       "273531          0  \n",
       "274029          0  \n",
       "\n",
       "[1962 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.post.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81f50fcabfa04e7f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:11.627328Z",
     "start_time": "2024-12-12T19:09:11.491104Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(df[df.post.isna()].index, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c553d9643715c",
   "metadata": {},
   "source": [
    "# TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:11.675968Z",
     "start_time": "2024-12-12T19:09:11.671496Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef5da25f9a5b60f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:29.535961Z",
     "start_time": "2024-12-12T19:09:11.684905Z"
    }
   },
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=1000)\n",
    "X_tfidf = tfidf.fit_transform(df.post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "16dc1a8576557779",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:29.586852Z",
     "start_time": "2024-12-12T19:09:29.555035Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['artificial', 'statistics', 'MachineLearning', 'computervision',\n",
       "       'rstats', 'analytics', 'datasets', 'computerscience',\n",
       "       'AskStatistics', 'data', 'datascience', 'MLQuestions',\n",
       "       'DataScienceJobs', 'deeplearning', 'dataengineering',\n",
       "       'dataanalysis', 'learnmachinelearning', 'kaggle',\n",
       "       'datascienceproject'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.subreddit.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61aed6a6b2b2ea98",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:09:29.748154Z",
     "start_time": "2024-12-12T19:09:29.686531Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(df.subreddit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b09912be56768a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a5e610e3ac19cc6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:11:00.915796Z",
     "start_time": "2024-12-12T19:11:00.909909Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<272241x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 7941020 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca4028fe9e743a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T10:28:25.752543Z",
     "start_time": "2024-12-11T10:28:25.613412Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tfidf, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7fc037a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<190568x1000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5562764 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ef1422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  7, 10, ...,  6,  0, 18], shape=(190568,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5431483f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T20:00:57.574106Z",
     "start_time": "2024-12-12T20:00:57.384008Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f2ec8d61dea357c9",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-12-11T10:28:25.768785Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'random_forest = RandomForestClassifier(n_jobs=-1, random_state=42)\\nrandom_forest.fit(X_train, Y_train)\\n#15 min '"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"random_forest = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "#15 min \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30b4dcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'logreg = LogisticRegression(max_iter=1000, n_jobs=-1)\\nlogreg.fit(X_train, Y_train)\\n#1 min'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"logreg = LogisticRegression(max_iter=1000, n_jobs=-1)\n",
    "logreg.fit(X_train, Y_train)\n",
    "#1 min\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09770719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenando: Gradient Boosting\n",
      "Tiempo de entrenamiento de Gradient Boosting: 31007.85s\n",
      "Gradient Boosting Accuracy: 0.46\n",
      "Gradient Boosting F1 Score: 0.45\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       AskStatistics       0.47      0.51      0.49      5989\n",
      "     DataScienceJobs       0.83      0.55      0.66       458\n",
      "         MLQuestions       0.21      0.03      0.05      2254\n",
      "     MachineLearning       0.34      0.55      0.42      7365\n",
      "           analytics       0.66      0.55      0.60      1611\n",
      "          artificial       0.53      0.45      0.49      1721\n",
      "     computerscience       0.57      0.65      0.60      4511\n",
      "      computervision       0.49      0.49      0.49      1853\n",
      "                data       0.57      0.20      0.30       544\n",
      "        dataanalysis       0.33      0.15      0.20       825\n",
      "     dataengineering       0.66      0.53      0.59      1648\n",
      "         datascience       0.51      0.59      0.55      7632\n",
      "  datascienceproject       0.00      0.00      0.00        57\n",
      "            datasets       0.53      0.59      0.56      2201\n",
      "        deeplearning       0.29      0.10      0.15      1654\n",
      "              kaggle       0.24      0.19      0.21        80\n",
      "learnmachinelearning       0.41      0.35      0.38      5958\n",
      "              rstats       0.52      0.49      0.51      2165\n",
      "          statistics       0.41      0.34      0.37      5923\n",
      "\n",
      "            accuracy                           0.46     54449\n",
      "           macro avg       0.45      0.38      0.40     54449\n",
      "        weighted avg       0.46      0.46      0.45     54449\n",
      "\n",
      "Entrenando: Naive Bayes\n",
      "Tiempo de entrenamiento de Naive Bayes: 0.96s\n",
      "Naive Bayes Accuracy: 0.44\n",
      "Naive Bayes F1 Score: 0.41\n",
      "                      precision    recall  f1-score   support\n",
      "\n",
      "       AskStatistics       0.45      0.67      0.54      5989\n",
      "     DataScienceJobs       0.97      0.49      0.65       458\n",
      "         MLQuestions       0.29      0.01      0.03      2254\n",
      "     MachineLearning       0.35      0.64      0.45      7365\n",
      "           analytics       0.83      0.39      0.53      1611\n",
      "          artificial       0.61      0.17      0.27      1721\n",
      "     computerscience       0.63      0.54      0.58      4511\n",
      "      computervision       0.60      0.27      0.37      1853\n",
      "                data       0.77      0.19      0.30       544\n",
      "        dataanalysis       0.00      0.00      0.00       825\n",
      "     dataengineering       0.72      0.28      0.40      1648\n",
      "         datascience       0.39      0.67      0.49      7632\n",
      "  datascienceproject       0.00      0.00      0.00        57\n",
      "            datasets       0.61      0.49      0.54      2201\n",
      "        deeplearning       0.75      0.00      0.00      1654\n",
      "              kaggle       0.00      0.00      0.00        80\n",
      "learnmachinelearning       0.39      0.34      0.36      5958\n",
      "              rstats       0.58      0.42      0.49      2165\n",
      "          statistics       0.36      0.21      0.27      5923\n",
      "\n",
      "            accuracy                           0.44     54449\n",
      "           macro avg       0.49      0.30      0.33     54449\n",
      "        weighted avg       0.47      0.44      0.41     54449\n",
      "\n",
      "El mejor modelo es: Gradient Boosting con una precisión de 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from xgboost import XGBClassifier\n",
    "import time\n",
    "\n",
    "# Datos de entrada y etiquetas\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tfidf, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Lista de modelos\n",
    "models = {\n",
    "    #\"Logistic Regression\": LogisticRegression(max_iter=1000, n_jobs=-1),\n",
    "    #\"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    #\"XGBoost\": XGBClassifier(n_estimators=100, tree_method='gpu_hist', max_depth=3, learning_rate=0.1, random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(max_depth=3,random_state=42),\n",
    "    \"Naive Bayes\": MultinomialNB()\n",
    "}\n",
    "\n",
    "# Probar cada modelo\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Entrenando: {model_name}\")\n",
    "    start = time.time()\n",
    "    model.fit(X_train, Y_train)  # Entrenar modelo\n",
    "    end = time.time()\n",
    "    Y_pred = model.predict(X_test)  # Predecir etiquetas\n",
    "    \n",
    "    # Evaluar el modelo\n",
    "    accuracy = accuracy_score(Y_test, Y_pred)\n",
    "    f1score = f1_score(Y_test, Y_pred, average='weighted')\n",
    "    print(f\"Tiempo de entrenamiento de {model_name}: {end - start:.2f}s\")\n",
    "    print(f\"{model_name} Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"{model_name} F1 Score: {f1score:.2f}\")\n",
    "    print(classification_report(Y_test, Y_pred, target_names=encoder.classes_))\n",
    "    # Guardar resultados\n",
    "    results[model_name] = accuracy\n",
    "\n",
    "# Mostrar el mejor modelo\n",
    "best_model = max(results, key=results.get)\n",
    "print(f\"El mejor modelo es: {best_model} con una precisión de {results[best_model]:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a550cc3da79c960",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-12T19:03:57.975115Z",
     "start_time": "2024-12-12T19:02:57.764659Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'logreg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 7. Evaluamos los modelos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m Y_pred2 \u001b[38;5;241m=\u001b[39m \u001b[43mlogreg\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#Y_pred = model.predict(X_test)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_score(Y_test, Y_pred2))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'logreg' is not defined"
     ]
    }
   ],
   "source": [
    "# 7. Evaluamos los modelos\n",
    "Y_pred2 = logreg.predict(X_test)\n",
    "#Y_pred = model.predict(X_test)\n",
    "#print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred2))\n",
    "print(\"Classification Report:\\n\", classification_report(Y_test, Y_pred2))\n",
    "#print(\"Classification Report:\\n\", classification_report(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f60a35ce4d0165",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T09:43:07.006565Z",
     "start_time": "2024-12-11T09:43:07.004079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4653528990431413\n",
      "Classification Report:\n",
      " 0.44672030785914674\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(Y_test, Y_pred))\n",
    "print(\"Classification Report:\\n\", f1_score(Y_test, Y_pred, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f6fc23d33b0691",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T09:43:10.238997Z",
     "start_time": "2024-12-11T09:43:10.235003Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c30a960f8476e4f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T09:47:00.926707Z",
     "start_time": "2024-12-11T09:46:59.893700Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3db98b9cf0f3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(df.post[0])\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
