{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b201bca559fa3f",
   "metadata": {},
   "source": [
    "## Generacion de resumenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b605a8af7178651b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:26.886170Z",
     "start_time": "2024-12-16T15:04:26.878882Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import RegexpTokenizer, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "2ad132edbb9ae22f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:30.809864Z",
     "start_time": "2024-12-16T15:04:26.993933Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\iocol\\AppData\\Local\\Temp\\ipykernel_13096\\2731419115.py:1: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_full = pd.read_csv('reddit_database_sentiment.csv', sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "df_full = pd.read_csv('reddit_database_sentiment.csv', sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "1b679aa6238d7009",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:30.846157Z",
     "start_time": "2024-12-16T15:04:30.830835Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_date</th>\n",
       "      <th>created_timestamp</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>author_created_utc</th>\n",
       "      <th>full_link</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>num_crossposts</th>\n",
       "      <th>subreddit_subscribers</th>\n",
       "      <th>post</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010-02-11 19:47:22</td>\n",
       "      <td>1265910442.0</td>\n",
       "      <td>analytics</td>\n",
       "      <td>So what do you guys all do related to analytic...</td>\n",
       "      <td>xtom</td>\n",
       "      <td>1.227476e+09</td>\n",
       "      <td>https://www.reddit.com/r/analytics/comments/b0...</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>There's a lot of reasons to want to know all t...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010-03-04 20:17:26</td>\n",
       "      <td>1267726646.0</td>\n",
       "      <td>analytics</td>\n",
       "      <td>Google's Invasive, non-Anonymized Ad Targeting...</td>\n",
       "      <td>xtom</td>\n",
       "      <td>1.227476e+09</td>\n",
       "      <td>https://www.reddit.com/r/analytics/comments/b9...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>I'm cross posting this from /r/cyberlaw, hopef...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-06 04:51:18</td>\n",
       "      <td>1294282278.0</td>\n",
       "      <td>analytics</td>\n",
       "      <td>DotCed - Functional Web Analytics - Tagging, R...</td>\n",
       "      <td>dotced</td>\n",
       "      <td>1.294282e+09</td>\n",
       "      <td>https://www.reddit.com/r/analytics/comments/ew...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>DotCed,a Functional Analytics Consultant, offe...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-19 11:45:30</td>\n",
       "      <td>1295430330.0</td>\n",
       "      <td>analytics</td>\n",
       "      <td>Program Details - Data Analytics Course</td>\n",
       "      <td>iqrconsulting</td>\n",
       "      <td>1.288245e+09</td>\n",
       "      <td>https://www.reddit.com/r/analytics/comments/f5...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Here is the program details of the data analyt...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-19 21:52:28</td>\n",
       "      <td>1295466748.0</td>\n",
       "      <td>analytics</td>\n",
       "      <td>potential job in web analytics... need to anal...</td>\n",
       "      <td>therewontberiots</td>\n",
       "      <td>1.278672e+09</td>\n",
       "      <td>https://www.reddit.com/r/analytics/comments/f5...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i decided grad school (physics) was not for me...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274234</th>\n",
       "      <td>2022-05-07 21:38:52</td>\n",
       "      <td>1651948732.0</td>\n",
       "      <td>rstats</td>\n",
       "      <td>Help interpretting lmer model output</td>\n",
       "      <td>seeking-stillness</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/rstats/comments/ukjiy...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64078.0</td>\n",
       "      <td>Hello! I am wonder how the following output wo...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274235</th>\n",
       "      <td>2022-05-07 22:13:52</td>\n",
       "      <td>1651950832.0</td>\n",
       "      <td>rstats</td>\n",
       "      <td>Medical stats book with R</td>\n",
       "      <td>Sweaty_Catch_4275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/rstats/comments/ukk7u...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64080.0</td>\n",
       "      <td>Can anybody recommend me a book with medical s...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274236</th>\n",
       "      <td>2022-05-08 00:38:50</td>\n",
       "      <td>1651959530.0</td>\n",
       "      <td>rstats</td>\n",
       "      <td>Markov chains with unequal sequence lengths</td>\n",
       "      <td>sebelly</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/rstats/comments/ukn1i...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64083.0</td>\n",
       "      <td>I'm trying to build a simple Markov chain. I h...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274237</th>\n",
       "      <td>2022-05-08 01:19:00</td>\n",
       "      <td>1651961940.0</td>\n",
       "      <td>rstats</td>\n",
       "      <td>view all available Rcpp::plugins</td>\n",
       "      <td>BOBOLIU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/rstats/comments/uknuh...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64084.0</td>\n",
       "      <td>How do I view all available Rcpp::plugins? Tha...</td>\n",
       "      <td>POSITIVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274238</th>\n",
       "      <td>2022-05-08 01:19:34</td>\n",
       "      <td>1651961974.0</td>\n",
       "      <td>rstats</td>\n",
       "      <td>Print only loadings in factanal</td>\n",
       "      <td>artgotframed</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.reddit.com/r/rstats/comments/uknuw...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64084.0</td>\n",
       "      <td>Hi everybody,\\n\\nI am currently doing a factor...</td>\n",
       "      <td>NEGATIVE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>274239 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               created_date created_timestamp  subreddit  \\\n",
       "0       2010-02-11 19:47:22      1265910442.0  analytics   \n",
       "1       2010-03-04 20:17:26      1267726646.0  analytics   \n",
       "2       2011-01-06 04:51:18      1294282278.0  analytics   \n",
       "3       2011-01-19 11:45:30      1295430330.0  analytics   \n",
       "4       2011-01-19 21:52:28      1295466748.0  analytics   \n",
       "...                     ...               ...        ...   \n",
       "274234  2022-05-07 21:38:52      1651948732.0     rstats   \n",
       "274235  2022-05-07 22:13:52      1651950832.0     rstats   \n",
       "274236  2022-05-08 00:38:50      1651959530.0     rstats   \n",
       "274237  2022-05-08 01:19:00      1651961940.0     rstats   \n",
       "274238  2022-05-08 01:19:34      1651961974.0     rstats   \n",
       "\n",
       "                                                    title             author  \\\n",
       "0       So what do you guys all do related to analytic...               xtom   \n",
       "1       Google's Invasive, non-Anonymized Ad Targeting...               xtom   \n",
       "2       DotCed - Functional Web Analytics - Tagging, R...             dotced   \n",
       "3                 Program Details - Data Analytics Course      iqrconsulting   \n",
       "4       potential job in web analytics... need to anal...   therewontberiots   \n",
       "...                                                   ...                ...   \n",
       "274234               Help interpretting lmer model output  seeking-stillness   \n",
       "274235                          Medical stats book with R  Sweaty_Catch_4275   \n",
       "274236        Markov chains with unequal sequence lengths            sebelly   \n",
       "274237                   view all available Rcpp::plugins            BOBOLIU   \n",
       "274238                    Print only loadings in factanal       artgotframed   \n",
       "\n",
       "        author_created_utc                                          full_link  \\\n",
       "0             1.227476e+09  https://www.reddit.com/r/analytics/comments/b0...   \n",
       "1             1.227476e+09  https://www.reddit.com/r/analytics/comments/b9...   \n",
       "2             1.294282e+09  https://www.reddit.com/r/analytics/comments/ew...   \n",
       "3             1.288245e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
       "4             1.278672e+09  https://www.reddit.com/r/analytics/comments/f5...   \n",
       "...                    ...                                                ...   \n",
       "274234                 NaN  https://www.reddit.com/r/rstats/comments/ukjiy...   \n",
       "274235                 NaN  https://www.reddit.com/r/rstats/comments/ukk7u...   \n",
       "274236                 NaN  https://www.reddit.com/r/rstats/comments/ukn1i...   \n",
       "274237                 NaN  https://www.reddit.com/r/rstats/comments/uknuh...   \n",
       "274238                 NaN  https://www.reddit.com/r/rstats/comments/uknuw...   \n",
       "\n",
       "        score  num_comments  num_crossposts  subreddit_subscribers  \\\n",
       "0         7.0           4.0             0.0                    NaN   \n",
       "1         2.0           1.0             0.0                    NaN   \n",
       "2         1.0           1.0             NaN                    NaN   \n",
       "3         0.0           0.0             NaN                    NaN   \n",
       "4         2.0           4.0             NaN                    NaN   \n",
       "...       ...           ...             ...                    ...   \n",
       "274234    1.0           0.0             0.0                64078.0   \n",
       "274235    1.0           0.0             0.0                64080.0   \n",
       "274236    1.0           0.0             0.0                64083.0   \n",
       "274237    1.0           0.0             0.0                64084.0   \n",
       "274238    1.0           0.0             0.0                64084.0   \n",
       "\n",
       "                                                     post sentiment  \n",
       "0       There's a lot of reasons to want to know all t...  NEGATIVE  \n",
       "1       I'm cross posting this from /r/cyberlaw, hopef...  NEGATIVE  \n",
       "2       DotCed,a Functional Analytics Consultant, offe...  NEGATIVE  \n",
       "3       Here is the program details of the data analyt...  NEGATIVE  \n",
       "4       i decided grad school (physics) was not for me...  POSITIVE  \n",
       "...                                                   ...       ...  \n",
       "274234  Hello! I am wonder how the following output wo...  NEGATIVE  \n",
       "274235  Can anybody recommend me a book with medical s...  POSITIVE  \n",
       "274236  I'm trying to build a simple Markov chain. I h...  NEGATIVE  \n",
       "274237  How do I view all available Rcpp::plugins? Tha...  POSITIVE  \n",
       "274238  Hi everybody,\\n\\nI am currently doing a factor...  NEGATIVE  \n",
       "\n",
       "[274239 rows x 13 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7607885d221b4685",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:30.949091Z",
     "start_time": "2024-12-16T15:04:30.945584Z"
    }
   },
   "outputs": [],
   "source": [
    "texto_prueba = df_full['post'][45]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb976e68325b73a",
   "metadata": {},
   "source": [
    "\n",
    "### Test 1 implementacion basica.\n",
    "\n",
    "Vamos a hacer una primera implementacion con un texto de prueba para ver si el algoritmo funciona correctamente.\n",
    "\n",
    "dividimos el proceso en los siguientes pasos:\n",
    "\n",
    "- preprocesamos el texto eliminando enlaces, numeros convertimos a minuscula, lamatizamos y tokenizamos en oraciones.\n",
    "- creamos un countvactorizer con las stopwords del texto completo preprocesado, de ese modo podemos ver la frecuencia de las palabras en el texto.\n",
    "- vamos a darle una puntuacion a cada oraciÃ³n en funcion de la frecuencia de las palabras en cada oraciÃ³n.\n",
    "- seleccionamos las oraciones con mayor puntuacion y las unimos para formar el resumen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8ecb20ec0547bd73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.028003Z",
     "start_time": "2024-12-16T15:04:31.020905Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Funcion auxiliar para obtener el POS de cada palabra\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Funcion para preprocesar el texto devuelve el texto completo preprocesado y las oraciones procesadas\"\"\"\n",
    "    \n",
    "    # Convertir a minÃºsculas\n",
    "    text = text.lower()\n",
    "\n",
    "    # Eliminar enlaces y nÃºmeros\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "    # Tokenizar manteniendo signos de puntuaciÃ³n\n",
    "    re_sent_tokenizer = RegexpTokenizer(r'\\w+|[.!?;:]')\n",
    "    tokens = re_sent_tokenizer.tokenize(text)\n",
    "\n",
    "    # Lematizar cada palabra con su POS\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(token)) for token in tokens]\n",
    "\n",
    "    # Unir tokens lematizados en un solo texto\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    # Tokenizar en frases\n",
    "    sentences = sent_tokenize(lemmatized_text)\n",
    "\n",
    "    return sentences, lemmatized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a706f33b52c54b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.128055Z",
     "start_time": "2024-12-16T15:04:31.118642Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_sentences = nltk.sent_tokenize(texto_prueba)\n",
    "procesed_sentences, cleaned_text = preprocess_text(texto_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b8c988ca3ddca970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.203722Z",
     "start_time": "2024-12-16T15:04:31.197101Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creamos un count vectorizer con las stopwords del texto completo preprocesado\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "cv = CountVectorizer(stop_words=stop_words)\n",
    "cv_matrix = cv.fit_transform([cleaned_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e3d9f79de3a5c565",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.276271Z",
     "start_time": "2024-12-16T15:04:31.273112Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtenemos las palabras Ãºnicas del texto\n",
    "unique_words = list(cv.vocabulary_.keys())\n",
    "# Creamos un diccionario de palabras y sus frecuencias\n",
    "word_freq = dict(zip(unique_words, cv_matrix.sum(axis=0).A1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "655a4a1fc9b4e2b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.348124Z",
     "start_time": "2024-12-16T15:04:31.344134Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calculamos la frecuencia de palabras en cada oraciÃ³n\n",
    "sentence_scores = {}\n",
    "for i, sentence in enumerate(procesed_sentences):\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in sentence:\n",
    "            if raw_sentences[i] in sentence_scores.keys():\n",
    "                sentence_scores[raw_sentences[i]] += freq\n",
    "            else:\n",
    "                sentence_scores[raw_sentences[i]] = freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1c72011e13a044ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.452322Z",
     "start_time": "2024-12-16T15:04:31.449345Z"
    }
   },
   "outputs": [],
   "source": [
    "puntuaciones = np.array(list(sentence_scores.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "310067c2e3c930d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.530960Z",
     "start_time": "2024-12-16T15:04:31.526885Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 10,  9, 10, 16, 10,  9,  1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puntuaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d6e3f8062679cd",
   "metadata": {},
   "source": [
    "No podemos seleccionar un numero de oraciones concreto ya que no sabemos cuantas oraciones tiene el texto, Por lo que nos vamos a quedar con las que tengan una puntuacion mayor al percentil 75 de modo que nos quedemos con las oraciones mas relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2bbdd54f11bb6857",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.601270Z",
     "start_time": "2024-12-16T15:04:31.597941Z"
    }
   },
   "outputs": [],
   "source": [
    "threshold = np.quantile(puntuaciones, 0.75)\n",
    "indices = np.where(puntuaciones > threshold)[0]\n",
    "slected_raw_sentences = [sent for i, sent in enumerate(raw_sentences) if i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bec32b26c980617c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.670542Z",
     "start_time": "2024-12-16T15:04:31.667933Z"
    }
   },
   "outputs": [],
   "source": [
    "sumary = \"\".join(slected_raw_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "b6fa50e21660cc10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.744962Z",
     "start_time": "2024-12-16T15:04:31.741156Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Until the latest version of Google Analytics, a friend of mine used to receive a report every month with a general overview of various metrics from his website.I basically want Analytics to email a standard report with metrics from the menus Audience, Traffic sources, etc.'"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b783b85320b05e",
   "metadata": {},
   "source": [
    "## Test 2 Analisis de los resultados.\n",
    "\n",
    "El test uno ha funcionado sin embargo vemos un problema a la hora de asignar las puntuaciones a las palabras ya que hay palabras que se repiten mucho pero que no estan aportando demasiado significado por lo que no tienen sentido que tengan una puntuacion tan alta. Asumiendo que los datos del dataset nos estans sesgados de ninguna manera podemos asumir que las palaras mas repetidas son las menos importantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6fe385aab0e3d156",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:31.813758Z",
     "start_time": "2024-12-16T15:04:31.811111Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vamos a hacer un analisis de las palabras mas comunes en el dataset para ver si podemos hacer algo al respecto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "45cd7ca99278a1ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:32.507681Z",
     "start_time": "2024-12-16T15:04:31.879569Z"
    }
   },
   "outputs": [],
   "source": [
    "df_full['post'] = df_full['post'].apply(lambda x: str(x))\n",
    "df_full['post'].sample(100)\n",
    "\n",
    "#reducto todos los strings de esta serie a un solo string\n",
    "texto_completo = \" \".join(df_full['post'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4fc820d48d8c8d23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:04:44.853861Z",
     "start_time": "2024-12-16T15:04:32.584989Z"
    }
   },
   "outputs": [],
   "source": [
    "re_word_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "words = re_word_tokenizer.tokenize(texto_completo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fd50684464721f4e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.299761Z",
     "start_time": "2024-12-16T15:04:44.924897Z"
    }
   },
   "outputs": [],
   "source": [
    "text = nltk.Text(words)\n",
    "fdist = nltk.FreqDist(text)\n",
    "most_common = fdist.most_common(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4e75e383c1475755",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.382290Z",
     "start_time": "2024-12-16T15:05:02.378861Z"
    }
   },
   "outputs": [],
   "source": [
    "lista_palaras_comunes = [word.lower() for word, freq in most_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c982927cacc4e141",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.451966Z",
     "start_time": "2024-12-16T15:05:02.446694Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use\n",
      "go\n",
      "look\n",
      "example\n",
      "found\n",
      "go\n",
      "new\n",
      "help\n",
      "set\n",
      "go\n",
      "want\n",
      "etc\n",
      "much\n",
      "like\n",
      "one\n",
      "help\n",
      "one\n",
      "someone\n",
      "point\n",
      "right\n",
      "thank\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([17,  5,  8,  6, 14,  7,  2])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobamos que de este modo las palabras que no aportan significado estan en la lista de palabras mas comunes\n",
    "\n",
    "sentence_scores = {}\n",
    "for i, sentence in enumerate(procesed_sentences):\n",
    "    for word, freq in word_freq.items():\n",
    "        if word in sentence:\n",
    "            if word in lista_palaras_comunes:\n",
    "                print(word)\n",
    "            elif raw_sentences[i] in sentence_scores.keys():\n",
    "                sentence_scores[raw_sentences[i]] += freq\n",
    "            else:\n",
    "                sentence_scores[raw_sentences[i]] = freq\n",
    "puntuaciones = np.array(list(sentence_scores.values()))\n",
    "puntuaciones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3ae15070b8ec6",
   "metadata": {},
   "source": [
    "Aunque el resultado no es perfecto ya que el dataset parece estar sesgado hacie el ambito cientifico si que sonseguimos quitar muchas palabras que no estana aportando mucho significado lo cual nos serÃ¡ util para la implementacion final."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c56f97f85c475b",
   "metadata": {},
   "source": [
    "## Implementacion final\n",
    "\n",
    "vamos a hacer una implementacion final en forma de funcion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "be1850556ce46c8e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.530611Z",
     "start_time": "2024-12-16T15:05:02.516132Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Until the latest version of Google Analytics, a friend of mine used to receive a report every month with a general overview of various metrics from his website.I basically want Analytics to email a standard report with metrics from the menus Audience, Traffic sources, etc.\n"
     ]
    }
   ],
   "source": [
    "def post_summarisation(text: str):\n",
    "    \"\"\"Function to summarize a text\"\"\"\n",
    "\n",
    "    raw_sentences = nltk.sent_tokenize(text)\n",
    "    procesed_sentences, cleaned_text = preprocess_text(text)\n",
    "\n",
    "    # Utilizamos un Countvectorizer con las stopwords del texto completo preprocesado\n",
    "    stop_words = stopwords.words('english')\n",
    "    cv = CountVectorizer(stop_words=stop_words)\n",
    "    cv_matrix = cv.fit_transform([cleaned_text])\n",
    "\n",
    "    # Obtner las palabras Ãºnicas del texto\n",
    "    unique_words = list(cv.vocabulary_.keys())\n",
    "    # Creamos un diccionario de palabras y sus frecuencias\n",
    "    word_freq = dict(zip(unique_words, cv_matrix.sum(axis=0).A1))\n",
    "\n",
    "    # Calculamos la frecuencia de palabras en cada oraciÃ³n\n",
    "    sentence_scores = {}\n",
    "    for i, sentence in enumerate(procesed_sentences):\n",
    "        for word, freq in word_freq.items():\n",
    "            if word in sentence and word not in lista_palaras_comunes:\n",
    "                raw_sentence = raw_sentences[i] if i < len(raw_sentences) else \"\"\n",
    "                if raw_sentence in sentence_scores.keys():\n",
    "                    sentence_scores[raw_sentence] += freq\n",
    "                else:\n",
    "                    sentence_scores[raw_sentence] = freq\n",
    "\n",
    "    puntuaciones = np.array(list(sentence_scores.values()))\n",
    "    threshold = np.quantile(puntuaciones, 0.75)\n",
    "    indices = np.where(puntuaciones > threshold)[0]\n",
    "    slected_raw_sentences = [sent for i, sent in enumerate(raw_sentences) if i in indices]\n",
    "\n",
    "    sumary = \"\".join(slected_raw_sentences)\n",
    "\n",
    "    return sumary\n",
    "\n",
    "# Ejemplo\n",
    "texto_prueba = df_full['post'][45]\n",
    "resumen = post_summarisation(texto_prueba)\n",
    "print(resumen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "124c5760e375768e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.597199Z",
     "start_time": "2024-12-16T15:05:02.594406Z"
    }
   },
   "outputs": [],
   "source": [
    "texto_prueba = df_full['post'][45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6273349a1565f4c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.670250Z",
     "start_time": "2024-12-16T15:05:02.660530Z"
    }
   },
   "outputs": [],
   "source": [
    "resumen = post_summarisation(texto_prueba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d324116871f5896",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.737890Z",
     "start_time": "2024-12-16T15:05:02.734416Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Until the latest version of Google Analytics, a friend of mine used to receive a report every month with a general overview of various metrics from his website.I basically want Analytics to email a standard report with metrics from the menus Audience, Traffic sources, etc.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resumen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2e035a84176424a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:02.810693Z",
     "start_time": "2024-12-16T15:05:02.801310Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = df_full['post'].sample(10).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "e0a8702c5871632c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-16T15:05:03.089736Z",
     "start_time": "2024-12-16T15:05:02.874257Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Texto original:\n",
      "\n",
      "The role that data analytics plays in modern business is becoming increasingly appreciated. According to one report, the per-dollar-spent ROI gained from using analytics &amp; increased from[ $10.66 in 2011 to $13.01 in 2014](https://nucleusresearch.com/research/single/analytics-pays-back-13-01-for-every-dollar-spent/). Working with analytics is one thing, but translating data-driven insights into useful work products is quite another. Thatâ€™s where[ data visualization](https://www.inzata.com/bi-encyclopedia/data-visualization/) enters the picture.[ Data visualization](https://www.inzata.com/bi-encyclopedia/data-visualization/) is an opportunity to go beyond dumping data into an Excel spreadsheet. With the right approach, data[ visualizations](https://www.inzata.com/bi-encyclopedia/visualizations/) can improve a companyâ€™s efficiency and effectiveness in the following ways.\n",
      "\n",
      "### Shorter and Better Meetings\n",
      "\n",
      "At many organizations, analytics need to be converted into work products that are then presented to stakeholders at meetings. How you choose to go about presenting the insights youâ€™ve gained can influence the meetings you have. Research from the American Management Association has shown that data[ visualizations](https://www.inzata.com/bi-encyclopedia/visualizations/) were able to:\n",
      "\n",
      "* Shorten meeting times by 24%\n",
      "* Provide 43% greater effectiveness in persuading audiences\n",
      "* Bring about 21% more consensus in decision-making\n",
      "* Improve problem-solving by 19%\n",
      "\n",
      "Simply put, coming into a meeting with effective data[ visualizations](https://www.inzata.com/bi-encyclopedia/visualizations/) makes a meeting faster and more useful. Bear in mind that modern[ data visualization](https://www.inzata.com/bi-encyclopedia/data-visualization/) techniques can yield a lot more than just a few pie, bar and line[ charts](https://www.inzata.com/bi-encyclopedia/charts/). Todayâ€™s[ data visualization](https://www.inzata.com/bi-encyclopedia/data-visualization/) techniques include producing items like:\n",
      "\n",
      "* Interactive dashboards\n",
      "* Real-time updates\n",
      "* Geographic data\n",
      "* 3-D maps\n",
      "* [Cloud](https://www.inzata.com/bi-encyclopedia/cloud/) and[ bubble charts](https://www.inzata.com/bi-encyclopedia/bubble-charts/)\n",
      "* Tree maps\n",
      "\n",
      "### Visual Learning\n",
      "\n",
      "Most human beings cannot listen to or read large amounts of data and readily make sense of what it really means. Human beings tend to benefit from having a sense of how things relate over time and through space, and visualization examples help. In visualization examples, an[ alluvial diagram](https://datavizproject.com/data-type/alluvial-diagram/) of events can help people understand how one thing flows from one place to a new one.\n",
      "\n",
      "For some sense of how visualization examples can help understanding, consider this[ diagram of asylum seeking in Europe](https://datavizproject.com/wp-content/uploads/2015/11/Sk%C3%A6rmbillede-2016-01-21-kl.-16.37.57.png). Hearing that certain groups are more likely to have their applications accepted based on their origin and destination is one thing. Conversely, being able to study a diagram that shows the flow of people and their acceptance and rejection statuses makes it easier to process the idea.\n",
      "\n",
      "There are four core data visualization tools that can be used to represent insights. These are:\n",
      "\n",
      "* Color\n",
      "* Shape\n",
      "* Visual movement\n",
      "* Spatial relationships\n",
      "\n",
      "Just being able to differentiate to color-coded data points may go a long way to increase your understanding of the meaning of a piece of research. A companyâ€™s data team might visualize questions about new and established customers, for example, by coloring new users with red dots and old users with blue dots. This can make it easier to follow along as you see how changes in the customer base have shifted over time. Compare that to trying to fish out data from a spreadsheet.\n",
      "\n",
      "### Long-Term Engagement\n",
      "\n",
      "Particularly in the era where data visualization tools like dashboards can be made available to everyone who has a phone, tablet or laptop, thereâ€™s a lot to be said for the engagement[ value](https://www.inzata.com/bi-encyclopedia/value/) of data[ visualizations](https://www.inzata.com/bi-encyclopedia/visualizations/). Letâ€™s say a CFO who was presented with a report at a meeting wants to refer back to materials from the session. Rather than having to sift through papers or ask someone to email them a particular slide, they can simply pull up the companyâ€™s data visualization tools and check the presentation there.\n",
      "\n",
      "More importantly, increased interactivity can keep decision-makers engaged with data. Being able to click on items and see how different factors shift can improve engagement significantly. Especially when working with parties that arenâ€™t 100% sold on your ideas, it can be helpful for them to scan and interact with data over several iterations.\n",
      "\n",
      "People also enjoy interacting with data. Switching back and forth using the data visualization tools between an operations current-year report and one from last year, for example, can foster engagement and interest.\n",
      "\n",
      "### Promoting Culture Change\n",
      "\n",
      "Becoming a data-centric organization requires bringing along decision-makers, employees, contractors, customers and other stakeholders. You want to onboard as many of these parties as possible as your company starts valuing data as a part of its decision-making process. Whenever possible, you also donâ€™t want to leave people behind.\n",
      "\n",
      "Data visualizations can help folks get onboard with a culture change thatâ€™s moving toward data and analytics. Improvements in engagement, learning and efficiency can help them feel why the culture change has to happen and how it benefits them.\n",
      "\n",
      "Stakeholders will eventually become more proficient as they settle into patterns of using visualizations. They will come to understand and apply statistical concepts such as:\n",
      "\n",
      "* Regression to the mean\n",
      "* Outliers\n",
      "* Hypothesis testing\n",
      "* Statistical confidence and uncertainty\n",
      "\n",
      "Theyâ€™ll also begin to appreciate why certain data visualization techniques were employed.\n",
      "\n",
      "Over time, analytics insights can become a product that stakeholders start to demand rather than dread seeing. People will whip out their phones and tablets to check up on the state of the company in real-time via dashboards. Instead of feeling like the culture change has been imposed upon them, they will start to see it as just something they canâ€™t do without.\n",
      "\n",
      "**Original Post:**[ https://www.inzata.com/4-reasons-to-utilize-data-visualization-software/](https://www.inzata.com/4-reasons-to-utilize-data-visualization-software/)\n",
      "\n",
      "Resumen:\n",
      "\n",
      "Research from the American Management Association has shown that data[ visualizations](https://www.inzata.com/bi-encyclopedia/visualizations/) were able to:\n",
      "\n",
      "* Shorten meeting times by 24%\n",
      "* Provide 43% greater effectiveness in persuading audiences\n",
      "* Bring about 21% more consensus in decision-making\n",
      "* Improve problem-solving by 19%\n",
      "\n",
      "Simply put, coming into a meeting with effective data[ visualizations](https://www.inzata.com/bi-encyclopedia/visualizations/) makes a meeting faster and more useful.Bear in mind that modern[ data visualization](https://www.inzata.com/bi-encyclopedia/data-visualization/) techniques can yield a lot more than just a few pie, bar and line[ charts](https://www.inzata.com/bi-encyclopedia/charts/).In visualization examples, an[ alluvial diagram](https://datavizproject.com/data-type/alluvial-diagram/) of events can help people understand how one thing flows from one place to a new one.For some sense of how visualization examples can help understanding, consider this[ diagram of asylum seeking in Europe](https://datavizproject.com/wp-content/uploads/2015/11/Sk%C3%A6rmbillede-2016-01-21-kl.-16.37.57.png).Conversely, being able to study a diagram that shows the flow of people and their acceptance and rejection statuses makes it easier to process the idea.This can make it easier to follow along as you see how changes in the customer base have shifted over time.Especially when working with parties that arenâ€™t 100% sold on your ideas, it can be helpful for them to scan and interact with data over several iterations.Improvements in engagement, learning and efficiency can help them feel why the culture change has to happen and how it benefits them.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Hi, I would like to filter out all data from Internet Explorer in google analytics and would like some help. Anyone know how to do this? It is skewing our results completely. \n",
      "Thanks in advance\n",
      "\n",
      "Resumen:\n",
      "\n",
      "Hi, I would like to filter out all data from Internet Explorer in google analytics and would like some help.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "# What are your prospects after achieving Tableau Desktop Certified Associate?\n",
      "\n",
      "Resumen:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Hi, my last assessment for my statistic class is supposed to be a one-sample t-test, however, it fails the criterions (i hope its the word for it, sorry i dont take the class in english).   \n",
      "The normality and even the Z-test are significant, so im at loss here. (N=253)  \n",
      "I dont want anyone to do the hw for me i just really need some advice, i tried to look it up, and tried everything, im just stupid.\n",
      "\n",
      "I post the assessment itself, maybe i overlooked something...\n",
      "\n",
      "Thank you in advance \n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "https://preview.redd.it/nrw0qsr29q461.png?width=649&amp;format=png&amp;auto=webp&amp;s=c4c70e721f26a67031ee9e5a9b708b9a06cec34e\n",
      "\n",
      "Resumen:\n",
      "\n",
      "Hi, my last assessment for my statistic class is supposed to be a one-sample t-test, however, it fails the criterions (i hope its the word for it, sorry i dont take the class in english).\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Hey guys , so i have been using labelImg to label my data , then using Tensorflow object detection API to make my object detection that consists of water bottles , remotes , pens.\n",
      "When i trained on those 3 items , it differentiates them accordingly. However it detects a water bottle as the label \"bottle\" that i have never trained on before , maybe because it is similarly looking to the bottle i trained on. Is there any way to solve this? Thanks yall.\n",
      "\n",
      "Resumen:\n",
      "\n",
      "Hey guys , so i have been using labelImg to label my data , then using Tensorflow object detection API to make my object detection that consists of water bottles , remotes , pens.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Been looking at old kaggle datasets to learn on (specifically the small ones since lack of GPU), and found this one -Â https://www.kaggle.com/c/predicting-a-rare-condition/data?select=health-diagnostics-train.csv\n",
      "\n",
      "\n",
      "I was trying to design a training dataset but realized I wasn't sure what kind of ratio of healthy/diseased to use in the training set. I was going to look at the performance of neural nets and different tree based methods, but I kinda arbitrarily chose a 1:3 ratio of diseased/healthy in the training set.\n",
      "\n",
      "\n",
      "Say I have 5000 \"diseased\" samples, but I know the true prevalence in the population is 1%, should I then build the training set with 5000 diseased/500,000 healthy?\n",
      "\n",
      "\n",
      "Couldn't find anything definitive on google but just wondering how people usually deal with this problem?\n",
      "\n",
      "Resumen:\n",
      "\n",
      "Been looking at old kaggle datasets to learn on (specifically the small ones since lack of GPU), and found this one -Â https://www.kaggle.com/c/predicting-a-rare-condition/data?select=health-diagnostics-train.csv\n",
      "\n",
      "\n",
      "I was trying to design a training dataset but realized I wasn't sure what kind of ratio of healthy/diseased to use in the training set.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Hi, I have to make a CNN to predict the results of a 7x7 tic tac toe game. Any ideas on how to get the data for the same ?\n",
      "\n",
      "Resumen:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "It seems more and more people are rushing to study machine learning while companies are hiring only a handful of them for research positions. Do you think there will be or is an oversupply of machine learning PhD? If one is interested in applying the machine learning models but not making a novel breakthrough in machine learning, do you think it would be better for one to go directly to the industry or another field and wait for machine learning researchers to make a breakthrough?\n",
      "\n",
      "Resumen:\n",
      "\n",
      "If one is interested in applying the machine learning models but not making a novel breakthrough in machine learning, do you think it would be better for one to go directly to the industry or another field and wait for machine learning researchers to make a breakthrough?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Who do you think would be more valuable to an employer in this field?\n",
      "\n",
      "Someone who only completed their undergrad in computer science but also did co-op\n",
      "\n",
      "Or\n",
      "\n",
      "Someone who didn't do a co-op but has a masters in computer science (thesis based, topic in data science and machine learning) and has done a couple of individual projects in their undergrad with a prof\n",
      "\n",
      "Thanks!\n",
      "\n",
      "Resumen:\n",
      "\n",
      "Someone who only completed their undergrad in computer science but also did co-op\n",
      "\n",
      "Or\n",
      "\n",
      "Someone who didn't do a co-op but has a masters in computer science (thesis based, topic in data science and machine learning) and has done a couple of individual projects in their undergrad with a prof\n",
      "\n",
      "Thanks!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Texto original:\n",
      "\n",
      "Dog sleeps, Dog drinks, Dog eats -  so basically dog is coming close to sleeps, drinks, eats...\n",
      "\n",
      "Cat sleeps, Cat drinks, Cat eats -  so here also cat is coming close to sleeps, drinks, eats...\n",
      "\n",
      "So the intuition is Dog is also coming close to Cat and it does visible after training the model\n",
      "\n",
      "Why and how this happens? Is there any formal proof?\n",
      "\n",
      "&amp;#x200B;\n",
      "\n",
      "P.S - window length is 2 and there are no such documents like, \" dog cat  or cat dog\"\n",
      "\n",
      "I believing ordering of word doesn't matter in Word2vec, as the name suggests \"Bag of words\", once you put in the bag, it's just random. \n",
      "\n",
      "Resumen:\n",
      "\n",
      "Is there any formal proof?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sample in samples:\n",
    "    resumen = post_summarisation(sample)\n",
    "    print(\"\\nTexto original:\\n\")\n",
    "    print(sample)\n",
    "    print(\"\\nResumen:\\n\")\n",
    "    print(resumen)\n",
    "    print(\"\\n\" * 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
